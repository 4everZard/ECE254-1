# ECE 254 Notes Part 3

### Lecture 19 - Memory

**Main Memory**

- CPU fetches and decodes instruction from memory, fetch operands from memory, store results back in memory
  - Simple instruction -> 4 memory accesses
- Developers assume:
  1. Main memory is unlimited
  2. All of main memory is available to program
- Most OSes manage memory

**No Memory Management**

- Up to 1980s, there was no memory management strategy
- Memory was treated like a linear array
- Programs directly accessed memory addresses and programs needed to state "start" address (location after OS and drivers) and "end" address
  - Problem: No coordination

**Solution**?

- On process switch, move all memory contents to disk
  - Problem: Very expensive (GBs of data)
  - Problem (remainder from original problem): OS is not protected and can be accessed by programs

**Solution Attempt 2 from IBM**

- Main memory divided into 2KB blocks, each with a 4-bit protection key
  - key held in special CPU registers
- Program Status Word (PSW) also contained 4-bit key
- Programs accessing memory with different 4-bit key than their PSW will result in errors
- Only OS can change protection key

**Generalized Solution 2**

- Each program gets "base" and "limit" addresses
  - Each memory access checks if `base < address_for_access < base + limit`
- Done in hardware because it's needed often and faster that way

![lect19-base_limit_check](.\Graphics\lect19-base_limit_check.PNG)

- Problem: Programs still reference absolute physical locations
  - When two processes loaded into memory from disk, then instructions might overlap (e.g `JMP` command)
  - ![lect19-abs_mem_problem](.\Graphics\lect19-abs_mem_problem.PNG)
  - Temporary Solution: Add base address of program to every address in the program when it's loaded
    - Problem: VERY SLOW
    - Problem: Hard to tell between number and address



**When to Assign Memory Location for Variable**

1. <u>Compile Time:</u> Convert variables to address locations at compile time (e.g. solution 1). Assembly and MS-DOS .COM does this.
2. <u>Load Time:</u> Addresses are updated when program is loaded (e.g. IBM solution)
   - Compiler needs to indicate what number are addresses so it can be updated
3. <u>Execution Time:</u> Do bindings at run-time
   - will require special hardware



**Address Space**

- A set of addresses that a process can use, independent of other processes' address spaces (unless shared memory)
  - abstraction layer
- For example, each process is given an area code and the instructions only need to dial the 7-digit phone number
- <u>Logical Address:</u> Address generated by CPU (the 7-digit phone number)
- <u>Physical Address:</u> actual location in memory (area-code)
- Done via hardware: <u>relocation register</u>![lect19-relocation_register](.\Graphics\lect19-relocation_register.PNG)
- This is <u>execution-time mapping</u>
- Pro: Program easily relocatable in memory (just change the relocation register value)
- Con: Less protection as no "limit" on logical and physical address
- Con: Additions are a lot slower than comparisons (due to carry propagation times)

**Swapping**

- Procedure of moving a process from memory to disk or vice-versa
- Very expensive to switch out entire process
- Modern OSes does modified form of swapping



### Lecture 20 - Dynamic Memory Allocation

**Generalized Interface**

- `void *allocate_memory (int size)` -- C equivalent of `malloc`
  - Allocate memory block of `size` bytes and return pointer of address to first byte
- `void *deallocate memory ( void *mem_block )` -- C equivalent of `free`
  - Deallocate the memory block `mem_block` 
  - Only indicates to OS that it is available for use, does not delete value
    - That's why you can `free` something and still be able to use that pointer
  - OS keeps track of every allocated block's size (how else does it know how much to deallocate?)
  - It is not possible to return a part of a block
- How to fulfill memory allocation requests?



**Case 1: Fixed Block Sizes**

- each block is a fixed size in bytes
- a program can request more than 1 block

Implementation:

1. <u>One Size for blocks</u> (e.g. 1KB)
   - Maintain linked list of available addresses
   - Allocation: grab head/tail
   - Deallocation: place deallocated address back into head/tail
2. <u>Multiple Fixed Block Sizes</u>
   - One linked list per fixed block size
   - Thus, multiple linked lists

Pros:

- Very fast for allocation ( constant time aka. O(1) )

Cons:

- <u>Internal Fragmentation:</u> unused memory that is internal to a partition
  - Some memory is wasted (e.g. program requests 1.5 blocks worth of memory so 2 blocks is allocated)
  - Bigger memory blocks -> bigger memory waste



**Case 2: Variable Block Size**

- Similar to fixed block size but have one smallest size possible (e.g. 1 KB)

Implementation:

1. <u>Bitmaps</u>

   - Given that memory has `M` units, each with `n` bits
   - Bitmap is bit array of length `M`. If the `kth` bit is 
     - 0, then the `kth` unit is unallocated
     - 1, then the `kth` unit has been allocated
   - Memory Overhead: `1/(n+1)`
     - For example, n == 4 bytes == 32 bits so overhead is `1/33 == 3%` 
     - 16 bytes == 128 bits so overhead is `1/129 == 0.8%`
   - To find a block of `k` bytes, we need to search bitmap for `8k/n` consecutive zeroes

2. <u>Doubly Linked Lists</u>

   - Each node has:
     - Start Address
     - Size of memory block
     - Allocated/Unallocated bit
   - Starts with one contiguous block of unallocated memory as only node
   - If a program requests 128 bytes, then we cut-out 128 bytes and create a new node
     - Linked list now has 2 nodes
     - Node 1: Start address, size == 128 bytes, bit == allocated
     - Node 2: Another start address, size == remaining size, bit == unallocated
   - If program deallocates memory, then bit will change from allocated to unallocated
   - <u>Coalescence:</u> Merging two or more adjacent unallocated blocks into 1 larger block
     - We need to do this because after a number of cuts, we might not have a continuous memory block large enough to support a request
     - performed periodically or when block of memory is freed
     - Check if `previous` and `next` of each free block is also free
     - Problem: Free memory blocks is spread all over main memory and can't be joined
   - <u>External Fragmentation:</u> When free memory is spread into little tiny fragments
     - External because the freed memory is not inside a memory block (as opposed to "internal fragmentation")
     - Solution 1 - Increase internal fragmentation
       - If there's extra memory in a block then requested, give it to the program anyways
       - Systems tend to round up to nearest power of 2
       - Problem: Doesn't solve external fragmentation, only decreases it
     - Solution 2 - <u>Compaction</u> (aka. <u>Relocation</u>)
       - Move all allocated memory next to one another
       - Problem: Very expensive
       - Problem: Impossible to do in some languages (like C) as we operate directly on memory addresses
     - Solution 3 - Using different allocation strategies

   

**Variable Allocation Strategies**

- Given request for size `N`, do we put it in 1st block that satisfies the condition we find or....
- Consider **two linked lists**: one for allocated and one for unallocated memory

Strategy 1 - First Fit

- Start at beginning of memory and check each block
- A block that is `> size N` is split into allocated block of size N and unallocated block of remaining size
- `O(n)` runtime, where `n` is # of blocks



Strategy 2 - Next Fit

- Similar to first fit but starts at beginning of where last block was allocated
- Prevents over-usage and concentration of external fragmentation at the start of memory
- `O(n)` runtime



Strategy 3 - Best Fit

- Consider all blocks and find the smallest block that is `> size N`
- If we check every block: `Theta(n)` runtime
- If we order blocks by increasing size: `O(n)`
  - If use AVL or red-black tree, then `Theta(ln(n))` runtime



Strategy 4 - Worst Fit

- Problem with best fit is that the "chopped" unallocated memory is probably useless
- This finds the biggest block that is `> size N` so that the "chopped" block can still be used
- Implementation: max heap or other heap



Strategy 5 - Quick Fit

- Optimization to other fitting strategies by keeping a list of blocks of fixed size (e.g. 1 MB) that are commonly requested
- ![lect20-allocation_strats](.\Graphics\lect20-allocation_strats.PNG)

**Choosing a Strategy**

1. First/Next fit
   - Fastest performance
   - Least wasted space
   - Next fit tends to do allocations at end of memory, breaking up the largest free memory block
2. Best Fit
   - Slower than first/next fit
   - Also least wasted space (space usage about the same)
3. Worst fit performs worst
   - Slowest Performance
   - Most wasted space

- Even with optimization, given `x` allocated blocks, up to `0.5x` blocks can be lost to fragmentation



**Binary Buddy**

- Memory blocks are available in powers of 2
- Some internal fragmentation, less external fragmentation

Given a memory request of size `n`,

1. Repeatedly split the size by half until we get the smallest block greater or equal to size `n`

2. In subsequent memory requests, we check:

   a. Is there already a memory block that meets the needs (smallest block `>= size n`)

   b. Can we subdivide a block to get there

3. Cleanup: A pair of buddies (2 blocks of equal size beside each other) can be coalesced if they are both free

![lect20-binary_buddy](.\Graphics\lect20-binary_buddy.PNG)

### Lecture 21 - Memory Segmentation and Paging

 <u>Segments:</u> memory 'elements' such as the stack, heap, standard C library

- Segments of a program (constructed by compiler)
  1. Code (aka instructions)
  2. Global Variables
  3. Heap
  4. Stack (one per thread)
  5. Standard C library
- Think of memory as a tuple `<segment, offset>`
  - Segment table maps tuple to pure memory addresses
  - Each entry in table has <u>base address</u> and <u>limit</u> (ie. size of segment)
- Hardware manages memory access (addition operation of base + offset) and memory verification (making sure offset < limit)

![lect21-memory_access](.\Graphics\lect21-memory_access.PNG)